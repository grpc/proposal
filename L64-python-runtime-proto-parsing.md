gRPC Python Runtime Proto Parsing
----
* Author(s): gnossen
* Approver: lidizheng
* Status: Draft
* Implemented in: Python
* Last updated: April 9, 2020
* Discussion at: **TODO**

## Abstract

Generated protobuf code is hard to manage for those without monorepos equipped with a hermetic build system. This has been echoed by the maintainers of libraries wrapping gRPC as well as direct users of gRPC. In this document, we aim to lay out a scalable solution for users of gRPC Python whose codebase consists of several independent code repositories.


## Background

gRPC and protocol buffers evolved in a Google ecosystem supported by a [monorepo](https://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/) and [hermetic build system](https://arxiv.org/pdf/1702.01715.pdf). When engineers check a `.proto` file into source control using that toolchain, the resultant code generated by the protocol compiler does not end up source control. Instead, the code is generated on demand during builds and cached in a distributed build artifact store. While such sophisticated machinery has started to become available in the open source community (e.g. [Bazel](https://bazel.build/)), it has not yet gained much traction. Instead, small Git repos and non-hermetic language-specific build tools are the norm. As a result, code generated by the protocol compiler is often checked into source contcontrol alongside the code which uses it.

At the least, this results in surprises when an update to a `.proto` file does not result in an update to the behavior of the userâ€™s code. However, when client and server code lives in separate repos, this can result in aliasing, where one repository houses generated code from an earlier version of the protocol buffer than the other.

Open source users are aware of this gap in the ecosystem and are actively looking for ways to fill it. Many have settled on [protocol buffer monorepos](https://medium.com/namely-labs/how-we-build-grpc-services-at-namely-52a3ae9e7c35) as a solution to the problem, wherein all `.proto` files for an organization are placed in a single source code repository and included by all other repositories as a submodule. But even this is not a complete solution. In addition, some mechanism must be put in place for the repositories housing the client and server code to retrieve the desired protocol buffer and generate code for the target language.

The protocol compiler paired with Google's build system also meant that the average engineer never has to manually invoke `protoc`. Instead, when an update is made to a `.proto` file and some code file references it, the the code for the protocol buffer is regenerated without any manual intervention on the part of the engineer. Compare that to today's workflow for gRPC users:

 1. Update .proto file.
 2. Manually regenerate code. (remembering how to use all of the CLI flags)
 3. Make necessary corresponding updates to code using the protocol buffer.
 4. Rerun.

It's easy for several of those steps to slip one's mind while developing. Moreover, figuring out how to invoke `protoc` in a way that meshes with your imports can be quite difficult. Python developers in particular are unused to build-time steps such as these; it is much more common to perform them at runtime.

### Related Proposals

* [Node.js gRPC+Protobuf.js Library](https://github.com/grpc/proposal/blob/master/L23-node-protobufjs-library.md)


## Proposal

Whereas today, users import protobuf code by invoking `protoc` to generate a file called `foo_pb2.py` and including the following line of code:

```python
import foo_pb2
import foo_pb2_grpc
```

They will now also have the option of completely skipping the manual protoc invocation and instead writing

```python
protos = grpc.protos('foo.proto')
services = grpc.services('foo.proto')
```

These two new functions return the same module objects as the `import foo_pb2` and `import foo_pb2_grpc` statements. In order to maintain interoperability with any proto-backed modules loaded in the same process, after these functions are invoked, `import foo_pb2` and `import foo_pb2_grpc` will be no-ops. That is, a side-effect of calling `grpc.protos` and `grpc.services` is insertion of the returned modules into the per-process cache. This ensures that regardless of whether the application calls `grpc.protos('foo.proto')` or `import foo_pb2` or in which order, only a single version of the module will ever be loaded into the process. This avoids situations in which interoperability breaks due to two modules expecting the same protobuf-level message type, but expecting two *different* `Message` classes, one backed by a `_pb2.py` file and one backed by a `.proto` file.

The wrapper function around the import servers several purposes here. First, it puts the user in control of naming the module (in a manner similar to Javascript), meaning the user never has to concern themself with the confusing `_pb2` suffix. Second, the function provides a wrapping layer through which the layer can provide guidance in the case of failed imports.

To be precise, we propose the introduction of three new functions with the following signatures:

```python
def protos(proto_file: Text,
           runtime_generated: Optional[bool] = True) -> types.ModuleType
def services(proto_file: Text,
             runtime_generated: Optional[bool] = True) -> types.ModuleType
def protos_and_services(proto_file: Text,
                        runtime_generated: Optional[bool] = True) -> Tuple[types.ModuleType, types.ModuleType]
```

The final function, `protos_and_services` is simple convenience function allowing the user to import protos and services in a single function call.

The change will be entirely backward compatible. Users manually invoking `protoc` today will not be required to change their build process.

## Import Paths

These functions will behave like normal `import` statements. `sys.path` will be used to search for `.proto` files. The path under which each particular `.proto` file was found will be passed to the protobuf parser as the root of the tree (equivalent to the `-I` flag of `protoc`). This means that a file located at `${SYS_PATH_ENTRY}/foo/bar/baz.proto` will result in the instantation of a module with the fully qualified name `foo.bar.baz_pb2`.  Users are expected to have a directory structure mirroring their desired import structure.


## Dependency Considerations

gRPC makes a point not to incur a direct dependency on protocol buffers. As such, this feature will not add a hard dependency on the `protobuf` package. Instead, the implementations of these new functions will live in the `grpcio-tools` package, which necessarily *already* has a hard dependency on `protobuf`. If the `grpcio` package finds that `grpc_tools` is importable, it will do so and use the implementations found there to back the `protos` and `services` functions. Otherwise, it will raise a `NotImplementedError`.

## Implementation

This proposal has been implemented [here](https://github.com/grpc/grpc/pull/21458).

This implementation uses the C++ protobuf runtime already built into the `grpcio-tools` C extension to parse the protocol buffers and generate textual Python code in memory. This code is then used to [instantiate the modules](https://www.python.org/dev/peps/pep-0302/) to be provided to the calling application.

## Alternatives Considered

Consideration was given to implementing the functionality of what is here presented as `grpc.protos` in the `protobuf` Python package. After a thorough investigation, we found that this feature could not be implemented in a way that satisfied both the compatibility requirements of the gRPC library and the Protobuf library. However, we are able to provide all of desired functionality with an implementation enirely in the `grpcio` package.
