Channel Tracing
----
* Author(s): Noah Eisen (ncteisen@google.com)
* Approver: markdroth, ctiller
* Status: Draft
* Implemented in: N/A
* Last updated: 03/09/17
* Discussion at: https://groups.google.com/forum/#!topic/grpc-io/WFDj3KeHYTI

## Table of Contents

  * [Abstract](#abstract)
  * [Background](#background)
  * [Proposal](#proposal)
  * [Rationale](#rationale)
  * [Implementation](#implementation)
     * [Basic Design](#basic-design)
     * [Tracer Functions](#tracer-functions)
     * [API Changes](#api-changes)
     * [Format of Exported Data](#format-of-exported-data)
     * [Garbage Collection](#garbage-collection)

## Abstract

The document proposes adding a dedicated, in-memory tracing object per channel and subchannel. The tracing object will record important steps in the life of a channel, like address resolution, subchannel creation, channel state changes, etc etc. This trace will be made available with a new gRPC API function call. The long term goal is to incorporate the trace into grpcz or a custom channel tracing UI.

## Background

Channel connectivity issues are the root cause of a significant portion of user reported gRPC bugs (for example, [here](https://groups.google.com/a/google.com/forum/?utm_medium=email&utm_source=footer#!msg/grpc-users/gPFYbRAex1A/VM7h5FWAAgAJ) and [here](https://groups.google.com/a/google.com/forum/#!topic/grpc-users/0bsOqrxYvRc)). At the moment, gRPC has an extensive logging/tracing system in place. However, when juggling hundreds or thousands of connections, the logs can grow too huge to be quickly useful when debugging channel connectivity issues. Often times it is difficult to determine what events led to changes in connectivity.

## Proposal

To implement channel tracing, we will introduce a dedicated, in-memory tracing object that is owned by each channel and subchannel. We will add a new function to the grpc channel api that allows library users to retrieve all of the trace for the channel as a JSON or protobuf object. This returned trace will eventually be hooked into a UI, or exported through the ongoing grpcz project, where it could be used for debugging channel issues.

## Rationale

This approach makes it easy to add trace, since each tracer is attached to its respective channel or subchannel. The tracer will also be passed down into the resolver and load balancer to record trace for interesting events that happen there. 

One downside to this approach is the potential for the tracer to bloat memory. For this reason, the logs will be held in circular buffers that cannot grow in size. At a certain point, old trace will be overwritten. This is described in more detail in the [Garbage Collection](#garbage-collection) section.

## Implementation

### Basic Design

The implementation will be done in the C-core first, with Java and Go to follow. The tracer object will hold a linked list of trace node and a linked list of subchannel tracers. Each of the subchannel tracers will hold their own linked lists of trace nodes.

```
struct grpc_trace_node {
  grpc_slice* data;
  grpc_error* error;
  gpr_timespec time;
  grpc_connectivity_state connectivity_state;
  grpc_trace_node* next;
};

struct trace_node_list {
  int size;
  grpc_trace_node* head_trace;
  grpc_trace_node* tail_trace;
  grpc_subchannel_tracer* referrenced_subchannel;
};

/* the channel tracing object */
struct grpc_channel_tracer {
  gpr_mu tracer_mu; 
  uint32_t num_nodes_logged;
  trace_node_list node_list;
  grpc_subchannel_tracer* head_subchannel;
  grpc_subchannel_tracer* tail_subchannel;
};

/* the subchannel tracing object */
struct grpc_subchannel_tracer {
  gpr_mu tracer_mu;
  gpr_refcount refs;
  trace_node_list node_list;
  uint32_t num_subchannels_logged;
  grpc_subchannel_tracer* next;
  grpc_subchannel_tracer* prev;
};
```
### Tracer Functions

The tracer files with expose several calls which with the C-core code can interact with tracers:

```
/* Initializes the tracing object with gpr_malloc. The caller has
   ownership over the returned tracing object */
grpc_channel_tracer* grpc_channel_tracer_init_tracer();
grpc_subchannel_tracer* grpc_subchannel_tracer_init_tracer();

/* Adds a new trace node to the tracing object */
void grpc_channel_tracer_add_trace(grpc_channel_tracer* tracer, 
    char* trace, struct grpc_error* error, gpr_timespec time, 
    grpc_connectivity_state connectivity_state);

/* Adds a subchannel to the tracing object */
void grpc_channel_tracer_add_subchannel(grpc_channel_tracer* tracer, 
    grpc_subchannel_tracer* subchannel);

/* Frees all of the resources held by the tracer object */
void grpc_channel_tracer_destroy_tracer();
```
### API Changes

And finally, the tracer will add a new method to the C-core API:
```
/* returns the tracing data in the form of a grpc json object */
grpc_json* grpc_channel_tracer_get_trace(grpc_channel_tracer* tracer);
```

### Format of Exported Data

The trace data will be exported as JSON from the C-core. However, it will be JSON that is of the correct form to be transformed to protobuf and back again. The JSON schema will be as follows:

```
{
  "numSubchannelsLogged":,
  "subchannels": [
    {
      "numNodesLogged": number,
      "nodes": [
        {
          "data": string,
          "error": string,
          "time": string
        },
      ]
    },
  ]
}
```

This matches the protobuf form of the exported trace:

```
// one single node of trace data
message TraceNode {
  // static string describing what event the channel went through
  string data = 1;
  // error assosiated with the channel event
  string error = 2;
  // time the trace was logged
  string time = 3;

  enum ConnectivityState {
    // channel has just been initialized
    GRPC_CHANNEL_INIT = 0;
    // channel is idle
    GRPC_CHANNEL_IDLE = 1;
    // channel is connecting
    GRPC_CHANNEL_CONNECTING = 2;
    // channel is ready for work
    GRPC_CHANNEL_READY = 3;
    // channel has seen a failure but expects to recover
    GRPC_CHANNEL_TRANSIENT_FAILURE = 4;
    // channel has seen a failure that it cannot recover from
    GRPC_CHANNEL_SHUTDOWN = 5;
  };
  // state the channel was in when the trace was logged
  ConnectivityState state = 4;
};

// trace for one subchannel
message SubchannelTrace {
  // tracks total trace nodes seen, since many may be overwritten
  int32 num_nodes_logged = 1;
  // all of the nodes for this subchannel
  repeated TraceNode nodes = 2;
}

// trace for one channel
message ChannelTrace {
  // tracks total subchannel seen, since some may be dropped as they go inactive
  int32 num_subchannels_logged = 1;
  // all of the subchannels for this channel
  repeated SubchannelTrace subchannels = 2;
};
```

### Garbage Collection

In order to ensure that the tracer(s) do not use too much memory, both trace nodes and subchannel tracers will be discarded after a time. All trace nodes will be stored in circular buffers, the size of which can be set by toggling the channel tracer channel arg.

At certain points a particular subchannel might stop being used. In order to efficiently discard old subchannels, we will use the following method: when the trace signaling the subchannel's death is about to be overwritten in the parent channel (overwritten because of the circular buffer), we will then discard and free the tracer for that subchannel.
